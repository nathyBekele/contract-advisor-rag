{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages/huggingface_hub-0.20.3-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pydantic-output-parser (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pydantic-output-parser\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydantic-output-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages/huggingface_hub-0.20.3-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain_openai\n",
      "  Downloading langchain_openai-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.26 (from langchain_openai)\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain_openai) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain_openai) (1.13.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (4.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (0.1.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.26->langchain_openai) (3.6)\n",
      "Requirement already satisfied: certifi in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.26->langchain_openai) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.26->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.26->langchain_openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.26->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.26->langchain_openai) (2.2.1)\n",
      "Downloading langchain_openai-0.0.7-py3-none-any.whl (33 kB)\n",
      "Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m784.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.24\n",
      "    Uninstalling langchain-core-0.1.24:\n",
      "      Successfully uninstalled langchain-core-0.1.24\n",
      "Successfully installed langchain-core-0.1.26 langchain_openai-0.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages/huggingface_hub-0.20.3-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting elasticsearch\n",
      "  Downloading elasticsearch-8.12.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting elastic-transport<9,>=8 (from elasticsearch)\n",
      "  Downloading elastic_transport-8.12.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from elastic-transport<9,>=8->elasticsearch) (2.2.1)\n",
      "Requirement already satisfied: certifi in /home/nati/miniconda3/envs/rag/lib/python3.12/site-packages (from elastic-transport<9,>=8->elasticsearch) (2024.2.2)\n",
      "Downloading elasticsearch-8.12.1-py3-none-any.whl (432 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.1/432.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading elastic_transport-8.12.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-dotenv, elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.12.0 elasticsearch-8.12.1 python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.vectorstores.elasticsearch import ElasticsearchStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import PyPDF2\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "es_model_id = '.elser_model_2_linux-x86_64'\n",
    "index_name = \"elser_index_vb_test_2\"\n",
    "llm_model_id = \"meta-llama/llama-2-13b-chat\"\n",
    "wx_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "wx_project_id = 'cc83aaaf-02bb-4d97-9a20-4ab4ce02cee6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs=[\"./Raptor Contract.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_docs(pdf_docs):\n",
    "    docs = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "        for index, text in enumerate(pdf_reader.pages):\n",
    "            doc_page = {'title': pdf + \" page \" + str(index + 1),\n",
    "                        'content': pdf_reader.pages[index].extract_text()}\n",
    "            docs.append(doc_page)\n",
    "    for doc in docs:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(content, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=256,\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_and_get_vector_store(split_docs):\n",
    "    vector_store = ElasticsearchStore(\n",
    "                    es_cloud_id= \"8d770dd7f5e74f5da7014ec9069249b8:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRhYjI5YzA5NmVlZmQ0OTgzOWM0YTdiYzdjMzVkOWFiYiRiNGM2ZDU4ZjA0MmM0YTEzYmRiMTgzMjhjY2NmZThmZA==\",\n",
    "                    es_api_key=\"c0p0VTFvMEJHY3FJNUtFVE5ST2I6SmxPZnNjRkRUWUt0SmRTV05senlqdw==\",\n",
    "                    index_name=index_name,\n",
    "                    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=es_model_id)\n",
    "                    )\n",
    "    documents = vector_store.from_documents(\n",
    "        split_docs,\n",
    "        es_cloud_id= \"8d770dd7f5e74f5da7014ec9069249b8:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRhYjI5YzA5NmVlZmQ0OTgzOWM0YTdiYzdjMzVkOWFiYiRiNGM2ZDU4ZjA0MmM0YTEzYmRiMTgzMjhjY2NmZThmZA==\",\n",
    "        es_api_key=\"c0p0VTFvMEJHY3FJNUtFVE5ST2I6SmxPZnNjRkRUWUt0SmRTV05senlqdw==\",\n",
    "        index_name=index_name,\n",
    "        strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=es_model_id)\n",
    "    )\n",
    "    print(\"Documents indexed and vector Store returned\")\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"[INST]You are a helpful, respectful, and honest assistant. \n",
    "Always answer as helpfully as possible, while being safe. Be brief in your answers. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If you don\\\\'\\''t know the answer to a question, please do not share false information. \\n Answer with no more than 150 words, in 2 or 3 sentences. If you cannot base your answer on the given document, please state that you do not have an answer.\\n\\n{question} Answer with no more than 200 words. If you cannot base your answer on the given document, please state that you do not have an answer. do not include a question in your response. dont prompt to make select correct answers[/INST]\"\"\"\n",
    "\n",
    "template = \"\"\"[INST]\n",
    "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
    "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
    "- Do not fabricate information or include questions in your responses.\n",
    "- do not prompt to select answers. do not ask me questions\n",
    "\n",
    "{question}\n",
    "\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#template = \"\"\"Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_conversation_chain(vector_store):\n",
    "    openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "    retriever = vector_store.as_retriever()\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
    "                          (llm=openai_llm,\n",
    "                           retriever=retriever,\n",
    "                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                           memory=memory,\n",
    "                           return_source_documents=True))\n",
    "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer_against_sources(response_answer, source_documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    similarity_threshold = 0.5  \n",
    "    source_texts = [doc.page_content for doc in source_documents]\n",
    "\n",
    "    answer_embedding = model.encode(response_answer, convert_to_tensor=True)\n",
    "    source_embeddings = model.encode(source_texts, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(answer_embedding, source_embeddings)\n",
    "\n",
    "\n",
    "    if any(score.item() > similarity_threshold for score in cosine_scores[0]):\n",
    "        return True  \n",
    "\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n",
      "Documents are split into 333 passages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error adding texts: 333 document(s) failed to index.\n",
      "First error reason: Could not find trained model [.elser_model_2_linux-x86_64]\n"
     ]
    },
    {
     "ename": "BulkIndexError",
     "evalue": "333 document(s) failed to index.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBulkIndexError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m get_text_chunks(content, metadata)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Documents are split into 3 passages\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mingest_and_get_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Documents indexed and vector Store returned\u001b[39;00m\n\u001b[1;32m      8\u001b[0m conversation_chain\u001b[38;5;241m=\u001b[39mget_conversation_chain(vectorstore)\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mingest_and_get_vector_store\u001b[0;34m(split_docs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mingest_and_get_vector_store\u001b[39m(split_docs):\n\u001b[1;32m      2\u001b[0m     vector_store \u001b[38;5;241m=\u001b[39m ElasticsearchStore(\n\u001b[1;32m      3\u001b[0m                     es_cloud_id\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8d770dd7f5e74f5da7014ec9069249b8:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRhYjI5YzA5NmVlZmQ0OTgzOWM0YTdiYzdjMzVkOWFiYiRiNGM2ZDU4ZjA0MmM0YTEzYmRiMTgzMjhjY2NmZThmZA==\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                     es_api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc0p0VTFvMEJHY3FJNUtFVE5ST2I6SmxPZnNjRkRUWUt0SmRTV05senlqdw==\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                     index_name\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[1;32m      6\u001b[0m                     strategy\u001b[38;5;241m=\u001b[39mElasticsearchStore\u001b[38;5;241m.\u001b[39mSparseVectorRetrievalStrategy(model_id\u001b[38;5;241m=\u001b[39mes_model_id)\n\u001b[1;32m      7\u001b[0m                     )\n\u001b[0;32m----> 8\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mes_cloud_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8d770dd7f5e74f5da7014ec9069249b8:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRhYjI5YzA5NmVlZmQ0OTgzOWM0YTdiYzdjMzVkOWFiYiRiNGM2ZDU4ZjA0MmM0YTEzYmRiMTgzMjhjY2NmZThmZA==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mes_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc0p0VTFvMEJHY3FJNUtFVE5ST2I6SmxPZnNjRkRUWUt0SmRTV05senlqdw==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mElasticsearchStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseVectorRetrievalStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mes_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments indexed and vector Store returned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/elasticsearch.py:1240\u001b[0m, in \u001b[0;36mElasticsearchStore.from_documents\u001b[0;34m(cls, documents, embedding, bulk_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m elasticsearchStore \u001b[38;5;241m=\u001b[39m ElasticsearchStore\u001b[38;5;241m.\u001b[39m_create_cls_from_kwargs(\n\u001b[1;32m   1237\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1238\u001b[0m )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;66;03m# Encode the provided texts and add them to the newly created index.\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m \u001b[43melasticsearchStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m elasticsearchStore\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/vectorstores.py:119\u001b[0m, in \u001b[0;36madd_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Delete by vector ID or other criteria.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        False otherwise, None if not implemented.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete method must be implemented by subclass.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maadd_texts\u001b[39m(\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    120\u001b[0m     texts: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    121\u001b[0m     metadatas: Optional[List[\u001b[38;5;28mdict\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts, texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/elasticsearch.py:1040\u001b[0m, in \u001b[0;36mElasticsearchStore.add_texts\u001b[0;34m(self, texts, metadatas, ids, refresh_indices, create_index_if_not_exists, bulk_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# the search_type doesn't require inference, so we don't need to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# embed the texts.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefresh_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_index_if_not_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_index_if_not_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbulk_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/elasticsearch.py:998\u001b[0m, in \u001b[0;36mElasticsearchStore.__add\u001b[0;34m(self, texts, embeddings, metadatas, ids, refresh_indices, create_index_if_not_exists, bulk_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         firstError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39merrors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    997\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst error reason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirstError\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreason\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 998\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo texts to add to index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/vectorstores/elasticsearch.py:981\u001b[0m, in \u001b[0;36mElasticsearchStore.__add\u001b[0;34m(self, texts, embeddings, metadatas, ids, refresh_indices, create_index_if_not_exists, bulk_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(requests) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 981\u001b[0m         success, failed \u001b[38;5;241m=\u001b[39m \u001b[43mbulk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstats_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrefresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbulk_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    989\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuccess\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and failed to add \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m texts to index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         )\n\u001b[1;32m    992\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded texts \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/elasticsearch/helpers/actions.py:521\u001b[0m, in \u001b[0;36mbulk\u001b[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[1;32m    520\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myield_ok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstreaming_bulk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_status\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# go through request-response pairs and detect failures\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mok\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstats_only\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/elasticsearch/helpers/actions.py:436\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(max_backoff, initial_backoff \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (attempt \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_process_bulk_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbulk_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbulk_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mok\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/elasticsearch/helpers/actions.py:355\u001b[0m, in \u001b[0;36m_process_bulk_chunk\u001b[0;34m(client, bulk_actions, bulk_data, raise_on_exception, raise_on_error, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _process_bulk_chunk_success(\n\u001b[1;32m    350\u001b[0m         resp\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    351\u001b[0m         bulk_data\u001b[38;5;241m=\u001b[39mbulk_data,\n\u001b[1;32m    352\u001b[0m         ignore_status\u001b[38;5;241m=\u001b[39mignore_status,\n\u001b[1;32m    353\u001b[0m         raise_on_error\u001b[38;5;241m=\u001b[39mraise_on_error,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/elasticsearch/helpers/actions.py:274\u001b[0m, in \u001b[0;36m_process_bulk_chunk_success\u001b[0;34m(resp, bulk_data, ignore_status, raise_on_error)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m ok, {op_type: item}\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BulkIndexError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document(s) failed to index.\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors)\n",
      "\u001b[0;31mBulkIndexError\u001b[0m: 333 document(s) failed to index."
     ]
    }
   ],
   "source": [
    "# Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality.\n",
    "content, metadata = prepare_docs(pdf_docs)\n",
    "# Content and metadata are extracted from the documents\n",
    "split_docs = get_text_chunks(content, metadata)\n",
    "# Documents are split into 3 passages\n",
    "vectorstore = ingest_and_get_vector_store(split_docs)\n",
    "# Documents indexed and vector Store returned\n",
    "conversation_chain=get_conversation_chain(vectorstore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
