{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTryansformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from htmlTemplate import css, bot_template, user_template\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = 'your-openai-api-key'\n",
    "\n",
    "llmtemplate = \"\"\"[INST]\n",
    "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
    "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
    "- Do not fabricate information or include questions in your responses.\n",
    "- do not prompt to select answers. do not ask me questions\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "def prepare_docs(pdf_docs):\n",
    "    docs = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "        print(pdf.name)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "        for index, text in enumerate(pdf_reader.pages):\n",
    "            doc_page = {'title': pdf.name + \" page \" + str(index + 1),\n",
    "                        'content': pdf_reader.pages[index].extract_text()}\n",
    "            docs.append(doc_page)\n",
    "    for doc in docs:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    return content, metadata\n",
    "\n",
    "\n",
    "def get_text_chunks(content, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=256,\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Split documents into {len(split_docs)} passages\")\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "def ingest_into_vectordb(split_docs):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    return db\n",
    "\n",
    "\n",
    "def get_conversation_chain(vectordb):\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_openai_gpt3(\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        max_tokens=200,\n",
    "        engine='text-davinci-002',\n",
    "        api_key=openai.api_key,\n",
    "        prompt_template=llmtemplate\n",
    "    )\n",
    "\n",
    "    print(\"Conversational Chain created for OpenAI's GPT-3 using the vector store\")\n",
    "    return conversation_chain\n",
    "\n",
    "def handle_userinput(user_question):\n",
    "    response = st.session_state.conversation({'question': user_question})\n",
    "    st.session_state.chat_history = response['chat_history']\n",
    "    \n",
    "    for i, message in enumerate(st.session_state.chat_history):\n",
    "        print(i)\n",
    "        if i % 2 == 0:\n",
    "            st.write(user_template.replace(\n",
    "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
    "        else:\n",
    "            print(message.content)\n",
    "            st.write(bot_template.replace(\n",
    "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
